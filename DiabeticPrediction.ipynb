{
 "cells": [
  {
   "cell_type": "code",
   "id": "ffb82d34-9752-40fa-a530-7b01aff08bdb",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-18T07:25:09.588146Z",
     "start_time": "2024-09-18T07:25:09.369289Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "categorical_cols = ['Age','Sex','Education','Income'] # here we will use OneHot (convert to numeric data)\n",
    "numerical_cols = ['BMI','MentHlth','PhysHlth','GenHlth'] # here we will use scaling to give a better representation\n",
    "# the rest of the columns are binary\n",
    "\n",
    "##Feature Engineering\n",
    "#BMI_PhysActivity\n",
    "# train_data['BMI_PhysActivity'] = train_data['BMI'] * train_data['PhysActivity']\n",
    "# test_data['BMI_PhysActivity'] = test_data['BMI'] * test_data['PhysActivity']\n",
    "\n",
    "\n",
    "# Binning BMI into categories\n",
    "# bins = [0, 18.5, 25, 30, 35, 100]\n",
    "# labels = ['Underweight', 'Normal', 'Overweight', 'Obese', 'Severely Obese']\n",
    "# train_data['BMI_binned'] = pd.cut(train_data['BMI'], bins=bins, labels=labels)\n",
    "# test_data['BMI_binned'] = pd.cut(test_data['BMI'], bins=bins, labels=labels)\n",
    "# \n",
    "# # You'll need to add 'BMI_binned' to your categorical columns list for OneHotEncoding\n",
    "# categorical_cols.append('BMI_binned')\n",
    "\n",
    "\n",
    "print(train_data.head())\n",
    " \n",
    "X = train_data.drop(columns=['Target'])  # Drop target column\n",
    "y = train_data['Target']  # Target column\n",
    "\n",
    "# X = X.head(10000)\n",
    "# y = y.head(10000)\n",
    "\n",
    "X_apply = test_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  HeartDiseaseorAttack  \\\n",
      "0   1       1         1          1   21       0       0                     0   \n",
      "1   2       1         0          1   26       1       0                     0   \n",
      "2   3       1         1          1   29       0       0                     1   \n",
      "3   4       1         1          1   27       0       0                     0   \n",
      "4   5       1         1          1   26       1       0                     0   \n",
      "\n",
      "   PhysActivity  Fruits  ...  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
      "0             1       1  ...            0        4         0         0   \n",
      "1             1       1  ...            0        3         0         0   \n",
      "2             0       0  ...            0        3        15         5   \n",
      "3             1       1  ...            0        2         0         0   \n",
      "4             0       0  ...            0        2         0         0   \n",
      "\n",
      "   DiffWalk  Sex  Age  Education  Income  Target  \n",
      "0         0    0   13          4       5       1  \n",
      "1         0    0   10          5       3       0  \n",
      "2         0    1   13          6       8       1  \n",
      "3         0    1    9          6       8       0  \n",
      "4         0    0    6          5       7       0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T07:35:56.927005400Z",
     "start_time": "2024-09-18T07:28:44.337102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "model = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=27)),\n",
    "    # ('classifier', RandomForestClassifier(\n",
    "    #     random_state=27, \n",
    "    #     class_weight='balanced', \n",
    "    #     criterion='entropy', \n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=14,\n",
    "    #     min_samples_split=4,\n",
    "    #     min_samples_leaf=6,\n",
    "    # )),\n",
    "    # ('classifier', lgb.LGBMClassifier(random_state=27)),\n",
    "    ('classifier', XGBClassifier(\n",
    "        scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), \n",
    "        random_state=27,\n",
    "        max_depth=10,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=3,\n",
    "        n_estimators=100,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=100,\n",
    "        subsample=0.6,\n",
    "    ))\n",
    "])\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [50, 100, 200],\n",
    "#     'classifier__max_depth': [None, 10, 20, 30],\n",
    "#     'classifier__min_samples_split': [2, 5, 10],\n",
    "#     'classifier__min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [100, 200, 500],\n",
    "#     'classifier__max_depth': [3, 6, 10],\n",
    "#     'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "#     'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#     'classifier__min_child_weight': [1, 3, 5],\n",
    "#     'classifier__gamma': [0, 0.1, 0.3],\n",
    "#     'classifier__reg_alpha': [0, 0.01, 0.1],\n",
    "#     'classifier__reg_lambda': [1, 10, 100]\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__learning_rate': [0.01, 0.1],\n",
    "#     'classifier__n_estimators': [100, 200],\n",
    "#     'classifier__max_depth': [10, 20, 30],\n",
    "#     'classifier__subsample': [0.8, 1.0],\n",
    "#     'classifier__colsample_bytree': [0.8, 1.0]\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(model, param_grid, cv=3, scoring='balanced_accuracy', verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best balanced accuracy score: \", grid_search.best_score_)"
   ],
   "id": "d7dabcfba598befe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=30, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=100, classifier__subsample=1.0; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7978\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__subsample=1.0; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7988\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 81466, number of negative: 81466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7984\n",
      "[LightGBM] [Info] Number of data points in the train set: 162932, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=30, classifier__n_estimators=100, classifier__subsample=0.8; total time=   4.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 63\u001B[0m\n\u001B[0;32m     54\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclassifier__learning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.1\u001B[39m],\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclassifier__n_estimators\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m200\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclassifier__colsample_bytree\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m1.0\u001B[39m]\n\u001B[0;32m     60\u001B[0m }\n\u001B[0;32m     62\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(model, param_grid, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 63\u001B[0m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters found: \u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_params_)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest balanced accuracy score: \u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_score_)\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1013\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1014\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1015\u001B[0m     )\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1019\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1023\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1572\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1573\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    958\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    960\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    961\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    962\u001B[0m         )\n\u001B[0;32m    963\u001B[0m     )\n\u001B[1;32m--> 965\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    977\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    984\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    987\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    988\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    134\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    886\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 888\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    891\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    892\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py:329\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model.\u001B[39;00m\n\u001B[0;32m    286\u001B[0m \n\u001B[0;32m    287\u001B[0m \u001B[38;5;124;03mFit all the transforms/samplers one after the other and\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;124;03m    This estimator.\u001B[39;00m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    328\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[1;32m--> 329\u001B[0m Xt, yt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py:265\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[1;34m(self, X, y, routed_params)\u001B[0m\n\u001B[0;32m    255\u001B[0m     X, fitted_transformer \u001B[38;5;241m=\u001B[39m fit_transform_one_cached(\n\u001B[0;32m    256\u001B[0m         cloned_transformer,\n\u001B[0;32m    257\u001B[0m         X,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    262\u001B[0m         params\u001B[38;5;241m=\u001B[39mrouted_params[name],\n\u001B[0;32m    263\u001B[0m     )\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(cloned_transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_resample\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 265\u001B[0m     X, y, fitted_transformer \u001B[38;5;241m=\u001B[39m \u001B[43mfit_resample_one_cached\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcloned_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\joblib\\memory.py:312\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py:1057\u001B[0m, in \u001B[0;36m_fit_resample_one\u001B[1;34m(sampler, X, y, message_clsname, message, params)\u001B[0m\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit_resample_one\u001B[39m(sampler, X, y, message_clsname\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1056\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m-> 1057\u001B[0m         X_res, y_res \u001B[38;5;241m=\u001B[39m \u001B[43msampler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit_resample\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1059\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m X_res, y_res, sampler\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\base.py:208\u001B[0m, in \u001B[0;36mBaseSampler.fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resample the dataset.\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \n\u001B[0;32m    189\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03m    The corresponding label of `X_resampled`.\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m--> 208\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\base.py:112\u001B[0m, in \u001B[0;36mSamplerMixin.fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    106\u001B[0m X, y, binarize_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X_y(X, y)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy_ \u001B[38;5;241m=\u001B[39m check_sampling_strategy(\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy, y, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampling_type\n\u001B[0;32m    110\u001B[0m )\n\u001B[1;32m--> 112\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m y_ \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    115\u001B[0m     label_binarize(output[\u001B[38;5;241m1\u001B[39m], classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(y)) \u001B[38;5;28;01mif\u001B[39;00m binarize_y \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    116\u001B[0m )\n\u001B[0;32m    118\u001B[0m X_, y_ \u001B[38;5;241m=\u001B[39m arrays_transformer\u001B[38;5;241m.\u001B[39mtransform(output[\u001B[38;5;241m0\u001B[39m], y_)\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:389\u001B[0m, in \u001B[0;36mSMOTE._fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    386\u001B[0m X_class \u001B[38;5;241m=\u001B[39m _safe_indexing(X, target_class_indices)\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnn_k_\u001B[38;5;241m.\u001B[39mfit(X_class)\n\u001B[1;32m--> 389\u001B[0m nns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn_k_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[:, \u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m    390\u001B[0m X_new, y_new \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_samples(\n\u001B[0;32m    391\u001B[0m     X_class, y\u001B[38;5;241m.\u001B[39mdtype, class_sample, X_class, nns, n_samples, \u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m    392\u001B[0m )\n\u001B[0;32m    393\u001B[0m X_resampled\u001B[38;5;241m.\u001B[39mappend(X_new)\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:886\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    883\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    884\u001B[0m         kwds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meffective_metric_params_\n\u001B[1;32m--> 886\u001B[0m     chunked_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpairwise_distances_chunked\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    888\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    889\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreduce_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meffective_metric_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_method \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mball_tree\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkd_tree\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m issparse(X):\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2172\u001B[0m, in \u001B[0;36mpairwise_distances_chunked\u001B[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[0m\n\u001B[0;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2171\u001B[0m     X_chunk \u001B[38;5;241m=\u001B[39m X[sl]\n\u001B[1;32m-> 2172\u001B[0m D_chunk \u001B[38;5;241m=\u001B[39m \u001B[43mpairwise_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (X \u001B[38;5;129;01mis\u001B[39;00m Y \u001B[38;5;129;01mor\u001B[39;00m Y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m   2174\u001B[0m     metric, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2175\u001B[0m ) \u001B[38;5;129;01mis\u001B[39;00m euclidean_distances:\n\u001B[0;32m   2176\u001B[0m     \u001B[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001B[39;00m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# i.e. \"l2\"\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     D_chunk\u001B[38;5;241m.\u001B[39mflat[sl\u001B[38;5;241m.\u001B[39mstart :: _num_samples(X) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2375\u001B[0m, in \u001B[0;36mpairwise_distances\u001B[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001B[0m\n\u001B[0;32m   2372\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m distance\u001B[38;5;241m.\u001B[39msquareform(distance\u001B[38;5;241m.\u001B[39mpdist(X, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n\u001B[0;32m   2373\u001B[0m     func \u001B[38;5;241m=\u001B[39m partial(distance\u001B[38;5;241m.\u001B[39mcdist, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m-> 2375\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_parallel_pairwise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1893\u001B[0m, in \u001B[0;36m_parallel_pairwise\u001B[1;34m(X, Y, func, n_jobs, **kwds)\u001B[0m\n\u001B[0;32m   1890\u001B[0m X, Y, dtype \u001B[38;5;241m=\u001B[39m _return_float_dtype(X, Y)\n\u001B[0;32m   1892\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(n_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 1893\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1895\u001B[0m \u001B[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001B[39;00m\n\u001B[0;32m   1896\u001B[0m fd \u001B[38;5;241m=\u001B[39m delayed(_dist_wrapper)\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:372\u001B[0m, in \u001B[0;36meuclidean_distances\u001B[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001B[0m\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m Y_norm_squared\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m (\u001B[38;5;241m1\u001B[39m, Y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m    367\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    368\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncompatible dimensions for Y of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mY\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mY_norm_squared of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moriginal_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    370\u001B[0m         )\n\u001B[1;32m--> 372\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_euclidean_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Hgskulen p Vestlandet\\Machine Learning Code\\DiabeticPrediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:410\u001B[0m, in \u001B[0;36m_euclidean_distances\u001B[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001B[0m\n\u001B[0;32m    408\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m XX\n\u001B[0;32m    409\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m YY\n\u001B[1;32m--> 410\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaximum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdistances\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;66;03m# Ensure that distances between vectors and themselves are set to 0.0.\u001B[39;00m\n\u001B[0;32m    413\u001B[0m \u001B[38;5;66;03m# This may not be the case due to floating point rounding errors.\u001B[39;00m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m Y:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "324e1991-fad6-42f1-8c01-e06e3b8ab939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T07:25:31.861011Z",
     "start_time": "2024-09-18T07:25:31.398810Z"
    }
   },
   "source": [
    "model.fit(X_train,y_train)\n",
    "# \n",
    "# joblib.dump(model, 'diabeticPredictor-XGB.joblib')\n",
    "# model = joblib.load('diabeticPredictor-XGB.joblib')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "score = balanced_accuracy_score(y_test, predictions)\n",
    "\n",
    "print(score)\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "threshold = 0.46\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_apply)[:, 1]\n",
    "predictions = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "# # Print classification report (for your validation data)\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate the model with the validation set\n",
    "y_pred_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "val_predictions = (y_pred_val_proba > threshold).astype(int)\n",
    "val_score = balanced_accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'Id': X_apply['Id'],  # Assuming 'Id' is the name of the ID column in X_apply\n",
    "#     'Target': predictions\n",
    "# })\n",
    "# \n",
    "# # Save the predictions to a CSV file\n",
    "# submission_df.to_csv('submission-xgb_17-09-24.csv', index=False)\n",
    "\n",
    "# Output evaluation metrics for validation data\n",
    "print(f\"Balanced Accuracy Score on Validation: {val_score}\")\n",
    "print(classification_report(y_val, val_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19861, number of negative: 122199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 203\n",
      "[LightGBM] [Info] Number of data points in the train set: 142060, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.139807 -> initscore=-1.816893\n",
      "[LightGBM] [Info] Start training from score -1.816893\n",
      "0.5432503673911095\n",
      "Balanced Accuracy Score on Validation: 0.5714564084183873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     24435\n",
      "           1       0.58      0.16      0.25      3977\n",
      "\n",
      "    accuracy                           0.87     28412\n",
      "   macro avg       0.73      0.57      0.59     28412\n",
      "weighted avg       0.84      0.87      0.83     28412\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
